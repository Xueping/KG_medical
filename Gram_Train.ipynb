{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gram_helpers import train_model, calculate_dimSize, get_rootCode, pad_matrix, build_tree\n",
    "from gram_module import GRAM as gram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_file = 'outputs/mimic'\n",
    "seq_file = 'outputs/mimic.seqs'\n",
    "label_file = 'outputs/mimic.3digitICD9.seqs'\n",
    "\n",
    "embd_dim_size = 100\n",
    "attn_dim_size = 100\n",
    "rnn_dim_size = 100\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves = calculate_dimSize(seq_file)\n",
    "num_classes = calculate_dimSize(label_file)\n",
    "num_ancestors = get_rootCode(tree_file+'.level2.pk') - num_leaves + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves_list = []\n",
    "ancestors_list = []\n",
    "for i in range(5, 0, -1):\n",
    "    leaves, ancestors = build_tree(tree_file + '.level' + str(i) + '.pk')\n",
    "    leaves_list.extend(leaves)\n",
    "    ancestors_list.extend(ancestors)\n",
    "\n",
    "seqs = pickle.load(open(seq_file, 'rb'))\n",
    "labels = pickle.load(open(label_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gram_model(leaves_list, ancestors_list, num_leaves, num_ancestors,\n",
    "                   embd_dim_size, attn_dim_size, rnn_dim_size, num_classes, device)\n",
    "# Send the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRAM(\n",
       "  (embed_init): Embedding(5623, 100)\n",
       "  (dag_attention): DAGAttention(\n",
       "    (linear1): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       "  (gru): GRUNet(\n",
       "    (gru): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (fc): Linear(in_features=100, out_features=942, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t embed_init.weight\n",
      "\t dag_attention.linear1.weight\n",
      "\t dag_attention.linear1.bias\n",
      "\t dag_attention.linear2.weight\n",
      "\t dag_attention.linear2.bias\n",
      "\t gru.gru.weight_ih_l0\n",
      "\t gru.gru.weight_hh_l0\n",
      "\t gru.gru.bias_ih_l0\n",
      "\t gru.gru.bias_hh_l0\n",
      "\t gru.gru.weight_ih_l1\n",
      "\t gru.gru.weight_hh_l1\n",
      "\t gru.gru.bias_ih_l1\n",
      "\t gru.gru.bias_hh_l1\n",
      "\t gru.fc.weight\n",
      "\t gru.fc.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = model.parameters()\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adadelta(params_to_update, lr=0.0001, momentum=0.9)\n",
    "# # Setup the loss fxn\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# Setup the loss fxn\n",
    "# criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gram_helpers import load_data\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ... \n",
      "done!!\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 479.9710, Duration: 69.00863933563232\n",
      "val Loss: 367.9665, Duration: 0.051615238189697266\n",
      "test Loss: 364.6855, Duration: 0.07424521446228027\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 269.6667, Duration: 68.89263582229614\n",
      "val Loss: 188.2605, Duration: 0.04938030242919922\n",
      "test Loss: 186.2393, Duration: 0.08147525787353516\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 150.5180, Duration: 68.90287613868713\n",
      "val Loss: 120.8479, Duration: 0.05173969268798828\n",
      "test Loss: 119.5378, Duration: 0.0677804946899414\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 106.1804, Duration: 69.24955630302429\n",
      "val Loss: 93.8826, Duration: 0.04978537559509277\n",
      "test Loss: 92.8466, Duration: 0.0735173225402832\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 86.2953, Duration: 69.3009614944458\n",
      "val Loss: 80.0565, Duration: 0.0503997802734375\n",
      "test Loss: 79.1619, Duration: 0.07654929161071777\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 75.4458, Duration: 69.01796674728394\n",
      "val Loss: 71.9194, Duration: 0.049913644790649414\n",
      "test Loss: 71.1055, Duration: 0.0703897476196289\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 68.7009, Duration: 68.93461012840271\n",
      "val Loss: 66.5779, Duration: 0.051360368728637695\n",
      "test Loss: 65.8163, Duration: 0.07772231101989746\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 64.1321, Duration: 68.9250500202179\n",
      "val Loss: 62.8747, Duration: 0.04660391807556152\n",
      "test Loss: 62.1488, Duration: 0.07073068618774414\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 60.9142, Duration: 69.26559162139893\n",
      "val Loss: 60.1165, Duration: 0.05072474479675293\n",
      "test Loss: 59.4158, Duration: 0.06866812705993652\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 58.4635, Duration: 68.8690812587738\n",
      "val Loss: 58.0418, Duration: 0.05293416976928711\n",
      "test Loss: 57.3605, Duration: 0.06902313232421875\n",
      "Training complete in 11m 30s\n",
      "Best val Loss: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = seqs\n",
    "val_loss_history = []\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = 0.0\n",
    "epoch_duration = 0.0\n",
    "\n",
    "print('Loading data ... ')\n",
    "train_set, valid_set, test_set = load_data(sequences, labels)\n",
    "data_dict = dict()\n",
    "data_dict['train'] = train_set\n",
    "data_dict['val'] = valid_set\n",
    "data_dict['test'] = test_set\n",
    "\n",
    "print('done!!')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        data_set = data_dict[phase]\n",
    "        n_batches = int(np.ceil(float(len(data_set[0])) / float(batch_size)))\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Iterate over data.\n",
    "        for index in random.sample(range(n_batches), n_batches):\n",
    "            batchX = data_set[0][index * batch_size:(index + 1) * batch_size]\n",
    "            batchY = data_set[1][index * batch_size:(index + 1) * batch_size]\n",
    "            x, y, mask, lengths = pad_matrix(batchX, batchY, num_leaves, num_classes)\n",
    "\n",
    "            batchX = torch.from_numpy(x).to(device)\n",
    "            batchY = torch.from_numpy(y).to(device)\n",
    "            lengths = torch.from_numpy(lengths).to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # Get model outputs and calculate loss\n",
    "                outputs = model(batchX, mask)\n",
    "\n",
    "                # Customise Loss function\n",
    "                logEps = 1e-8\n",
    "                cross_entropy = -(batchY * torch.log(outputs + logEps) +\n",
    "                                  (1. - batchY) * torch.log(1. - outputs + logEps))\n",
    "                loglikelihood = cross_entropy.sum(axis=2).sum(axis=1) / lengths\n",
    "                loss = torch.mean(loglikelihood)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    # loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "        duration = time.time() - start_time\n",
    "        epoch_loss = running_loss / n_batches\n",
    "        print('{} Loss: {:.4f}, Duration: {}'.format(phase, epoch_loss, duration))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if phase == 'val':\n",
    "            val_loss_history.append(epoch_loss)\n",
    "        if phase == 'train':\n",
    "            epoch_duration += duration\n",
    "\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(epoch_duration // 60, epoch_duration % 60))\n",
    "print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "# load best model weights\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, hist = train_model(model, seqs, labels, criterion, optimizer, device, batch_size, num_epochs, num_leaves, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
